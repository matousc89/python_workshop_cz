{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python web scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Získání obsahu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://en.wikipedia.org/wiki/Penguin\"\n",
    "\n",
    "r = requests.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zpracování obsahu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_from_response(response):\n",
    "    \n",
    "    def handle_p(element):\n",
    "        if element.text == \"\\n\":\n",
    "            return []\n",
    "        for item in element.findAll(\"sup\"):\n",
    "            item.decompose() \n",
    "        clean_text = element.text.replace(\"\\n\", \"\")\n",
    "        return [(element.name, clean_text), ]\n",
    "\n",
    "    def handle_h(element):\n",
    "        for item in element.findAll(\"span\", {\"class\": \"mw-editsection\"}):\n",
    "            item.decompose()\n",
    "        return [(element.name, element.text), ]\n",
    "    \n",
    "    html = r.text\n",
    "    tree = BeautifulSoup(html, \"html.parser\")\n",
    "    main = tree.find(\"div\", {\"class\": \"mw-parser-output\"})\n",
    "    outcome = {\n",
    "        \"url\": response.url,\n",
    "        \"title\": tree.find(\"h1\").text,\n",
    "    }\n",
    "    \n",
    "    # content\n",
    "    content = []\n",
    "    for element in main.findAll(recursive=False):\n",
    "        if element.name == \"p\":\n",
    "            content += handle_p(element)\n",
    "        elif element.name.startswith(\"h\"):\n",
    "            content += handle_h(element)\n",
    "        elif element.name in [\"div\", \"span\", \"table\", \"ul\", \"ol\"]:\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "    outcome[\"content\"] = content\n",
    "    \n",
    "    # links\n",
    "    links = []\n",
    "    for link in main.findAll(\"a\"):\n",
    "        if \"href\" in link.attrs and not \":\" in link.attrs[\"href\"] and link.attrs[\"href\"].startswith(\"/wiki/\"):\n",
    "            links.append(link.attrs[\"href\"])\n",
    "    outcome[\"links\"] = links\n",
    "        \n",
    "    return outcome\n",
    "       \n",
    "       \n",
    "data = get_content_from_response(r)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ukládání obsahu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content2txt(data):\n",
    "    return \"\\n\".join([content[\"title\"],] + [item[1] for item in content[\"content\"]])\n",
    "\n",
    "def content2md(content):\n",
    "    output = \"# \" + content[\"title\"] + \"\\n\"\n",
    "    for item in content[\"content\"]:\n",
    "        if item[0] == \"p\":\n",
    "            output += item[1] + \"\\n\"\n",
    "        else:\n",
    "            level = int(item[0][1:])\n",
    "            output += (\"#\" * level) + \" \" + item[1] + \"\\n\"\n",
    "    return output\n",
    "    \n",
    "def content2html(content):\n",
    "    with open(os.path.join(\"data\", \"template1.html\"), \"r\") as f:\n",
    "        template = f.read()\n",
    "    output = \"<h1>\" + content[\"title\"] + \"</h1>\"\n",
    "    for item in content[\"content\"]:\n",
    "        output += \"<{0}>{1}</{0}>\".format(item[0], item[1])  \n",
    "    output = template.replace(\"[[BODY]]\", output).replace(\"[[TITLE]]\", content[\"title\"])\n",
    "    return output\n",
    "\n",
    "def save_data(data, extension, func):\n",
    "    to_write = func(data)\n",
    "    filename = data[\"title\"] + \"_\" + str(uuid.uuid1()) +\".\" + extension\n",
    "    savepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    with open(savepath, \"w\") as f:\n",
    "        f.write(to_write)   \n",
    "    \n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "save_data(data, \"html\", content2html)\n",
    "save_data(data, \"md\", content2md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatizace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 Penguin https://en.wikipedia.org/wiki/Penguin\n",
      "200 Penguin (disambiguation) https://en.wikipedia.org/wiki/Penguin_(disambiguation)\n",
      "200 Paleocene https://en.wikipedia.org/wiki/Paleocene\n",
      "200 Year https://en.wikipedia.org/wiki/Megaannum\n",
      "200 Precambrian https://en.wikipedia.org/wiki/Precambrian\n",
      "200 Cambrian https://en.wikipedia.org/wiki/Cambrian\n",
      "200 Ordovician https://en.wikipedia.org/wiki/Ordovician\n",
      "200 Silurian https://en.wikipedia.org/wiki/Silurian\n",
      "200 Devonian https://en.wikipedia.org/wiki/Devonian\n",
      "200 Carboniferous https://en.wikipedia.org/wiki/Carboniferous\n",
      "200 Permian https://en.wikipedia.org/wiki/Permian\n",
      "\n",
      "\n",
      "Visited:  11\n",
      "In queue:  4057\n"
     ]
    }
   ],
   "source": [
    "HOST = \"https://en.wikipedia.org\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "slugs = [\"/wiki/Penguin\", ]\n",
    "visited = []\n",
    "\n",
    "session = requests.session()\n",
    "\n",
    "\n",
    "idx = 0\n",
    "while slugs:\n",
    "    if not slugs[0] in visited:        \n",
    "        url = HOST + slugs[0]\n",
    "        visited.append(slugs[0])\n",
    "        r = session.get(url)\n",
    "        data = get_content_from_response(r)\n",
    "        slugs += data[\"links\"]\n",
    "        save_data(data, \"html\", content2html)\n",
    "        save_data(data, \"md\", content2md)\n",
    "        print(r.status_code, data[\"title\"], data[\"url\"])    \n",
    "        sleep_time = random.random() * 2 + 0.3\n",
    "        time.sleep(sleep_time)\n",
    "    else:\n",
    "        print(\"XXX\", data[\"title\"], data[\"url\"])\n",
    "    \n",
    "    del slugs[0]\n",
    "    idx += 1\n",
    "    if idx > 10:\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Visited: \", len(visited))\n",
    "print(\"In queue: \", len(slugs))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
